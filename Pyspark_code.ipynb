{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b7d2b6-bd76-4c4c-a6b8-a0b1275616ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "2.How many fatal log entries that occurred on a Tuesday or Thursday resulted from a ”machine check interrupt”?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f78f420b-2434-4ead-bc60-653128a28d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/29 14:53:53 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of fatal log entries on Tuesday or Thursday due to a machine check interrupt: 83\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Configuration variables\n",
    "LOG_FILE_PATH = \"file:///home/hduser/notebooks/BGL.log\"  # Update the path to the log file as needed\n",
    "APP_NAME = \"FatalLogCount\"  # Name of the Spark application\n",
    "SPARK_MASTER = \"local[*]\"  # Spark master configuration\n",
    "SPARK_DRIVER_HOST = \"localhost\"  # Spark driver host\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(APP_NAME) \\\n",
    "    .config(\"spark.driver.host\", SPARK_DRIVER_HOST) \\\n",
    "    .master(SPARK_MASTER) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the log file into an RDD (Resilient Distributed Dataset)\n",
    "log_rdd = spark.sparkContext.textFile(LOG_FILE_PATH)\n",
    "\n",
    "# Function to check if the log entry is fatal and occurs on Tuesday or Thursday due to a machine check interrupt\n",
    "def extract_fatal_errors(line):\n",
    "    columns = line.split()  # Split the log line into columns\n",
    "    if len(columns) > 5:  # Ensure there are enough columns to process\n",
    "        date_str = columns[2]  # Extract the date string from the log entry\n",
    "        message = \" \".join(columns[6:])  # Extract the log message\n",
    "        try:\n",
    "            date = datetime.datetime.strptime(date_str, \"%Y.%m.%d\")  # Parse the date string\n",
    "            day_of_week = date.weekday()  # Get the day of the week (0 = Monday, 6 = Sunday)\n",
    "            if day_of_week in [1, 3]:  # Check if it's Tuesday (1) or Thursday (3)\n",
    "                if \"machine check interrupt\" in message and \"FATAL\" in message:  # Check if the message contains specific keywords\n",
    "                    return 1  # Return 1 for valid fatal errors\n",
    "        except ValueError:\n",
    "            return None  # Return None if there's a date parsing error\n",
    "    return None  # Return None if conditions are not met\n",
    "\n",
    "# Extract relevant data by mapping and filtering the log entries\n",
    "fatal_error_rdd = log_rdd.map(extract_fatal_errors).filter(lambda x: x is not None)\n",
    "\n",
    "# Count the number of fatal log entries that meet the criteria\n",
    "fatal_error_count = fatal_error_rdd.count()\n",
    "\n",
    "# Print the count of fatal log entries\n",
    "print(f\"Number of fatal log entries on Tuesday or Thursday due to a machine check interrupt: {fatal_error_count}\")\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010fd233-f3cb-4e3c-a121-78a15a03542d",
   "metadata": {},
   "outputs": [],
   "source": [
    "7.For each week, what is the average number of times ”ddr errors” were detected and corrected? Assume a week runs\n",
    "from Monday to Sunday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a373be3-5e16-41a2-a4a9-101e9710771a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|avg_ddr_errors_per_week|\n",
      "+-----------------------+\n",
      "|                33939.0|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, weekofyear, count, avg, split\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"LogAnalysis\") \\\n",
    "    .config(\"spark.driver.host\", \"localhost\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read the log file into a DataFrame\n",
    "df_logs = spark.read.text(\"BGL.log\")\n",
    "\n",
    "# Define a function to parse the date and message from log lines\n",
    "def parse_log_line(line):\n",
    "    parts = line.split(\" \")\n",
    "    if len(parts) > 4:\n",
    "        date_str = parts[2]  # Extract the date part from the line\n",
    "        message = \" \".join(parts[3:])  # Combine the rest as the message\n",
    "        return (date_str, message)\n",
    "    return None\n",
    "\n",
    "# Register UDF (User Defined Function) to parse log lines\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType, StructType, StructField, DateType\n",
    "\n",
    "# Define the schema for the parsed data\n",
    "schema = StructType([\n",
    "    StructField(\"date\", StringType(), True),\n",
    "    StructField(\"message\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create a UDF with the defined schema\n",
    "parse_log_udf = udf(lambda line: parse_log_line(line), schema)\n",
    "\n",
    "# Apply the UDF to create a structured DataFrame with 'date' and 'message' columns\n",
    "df_parsed = df_logs.select(parse_log_udf(col(\"value\")).alias(\"parsed\")) \\\n",
    "    .select(\"parsed.*\") \\\n",
    "    .filter(col(\"message\").contains(\"ddr errors\"))  # Filter rows containing 'ddr errors'\n",
    "\n",
    "# Convert 'date' column to DateType and extract the week of the year\n",
    "df_parsed = df_parsed.withColumn(\"date\", col(\"date\").cast(DateType()))\n",
    "\n",
    "# Group by week of the year and count occurrences of DDR errors\n",
    "df_weekly_ddr_errors = df_parsed.groupBy(weekofyear(col('date')).alias('week')) \\\n",
    "    .agg(count('*').alias('ddr_error_count'))\n",
    "\n",
    "# Calculate the average number of DDR errors per week\n",
    "df_weekly_avg = df_weekly_ddr_errors.agg(avg('ddr_error_count').alias('avg_ddr_errors_per_week'))\n",
    "\n",
    "# Show the result\n",
    "df_weekly_avg.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c22b23-fc50-4887-bdcb-983ba3e36335",
   "metadata": {},
   "outputs": [],
   "source": [
    "10. What are the top 3 most frequently occurring days of the week in the log?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd6d80fe-b6c5-47eb-a38f-4aabd676b725",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 most frequently occurring days of the week:\n",
      "+-----------+------+\n",
      "|day_of_week| count|\n",
      "+-----------+------+\n",
      "|   Saturday|932934|\n",
      "|   Thursday|914707|\n",
      "|    Tuesday|829180|\n",
      "+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Configuration variables\n",
    "LOG_FILE_PATH = \"file:///home/hduser/notebooks/BGL.log\"  # Path to the log file\n",
    "APP_NAME = \"TopDaysOfWeek\"  \n",
    "SPARK_MASTER = \"local[*]\"  \n",
    "SPARK_DRIVER_HOST = \"localhost\"  # Spark driver host\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(APP_NAME) \\\n",
    "    .config(\"spark.driver.host\", SPARK_DRIVER_HOST) \\\n",
    "    .master(SPARK_MASTER) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read the log file into an RDD\n",
    "log_rdd = spark.sparkContext.textFile(LOG_FILE_PATH)\n",
    "\n",
    "# Function to extract day of the week from log entry\n",
    "def extract_day_of_week(line):\n",
    "    columns = line.split()  # Split the line into columns\n",
    "    if len(columns) > 5:  # Check if the line has more than 5 columns\n",
    "        date_str = columns[2]  # Extract the date string\n",
    "        try:\n",
    "            # Parse the date string into a datetime object\n",
    "            date = datetime.datetime.strptime(date_str, \"%Y.%m.%d\")\n",
    "            # Get the full weekday name (e.g., 'Monday')\n",
    "            day_of_week = date.strftime('%A')\n",
    "            return (day_of_week, 1)  # Return a tuple of the day and a count of 1\n",
    "        except ValueError:\n",
    "            return None  # Return None if date parsing fails\n",
    "    return None  # Return None if the line does not have more than 5 columns\n",
    "\n",
    "# Extract day of the week from each log entry and filter out None values\n",
    "days_rdd = log_rdd.map(extract_day_of_week).filter(lambda x: x is not None)\n",
    "\n",
    "# Count occurrences of each day of the week\n",
    "day_counts_rdd = days_rdd.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Get the top 3 most frequently occurring days\n",
    "top_days = day_counts_rdd.takeOrdered(3, key=lambda x: -x[1])\n",
    "\n",
    "# Convert the result to a DataFrame\n",
    "top_days_df = spark.createDataFrame(top_days, [\"day_of_week\", \"count\"])\n",
    "\n",
    "# Print the message\n",
    "print(\"Top 3 most frequently occurring days of the week:\")\n",
    "\n",
    "# Show the result in the required format\n",
    "top_days_df.orderBy(col(\"count\").desc()).show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abb6073-923a-4cf9-a7c8-ef61b93f8613",
   "metadata": {},
   "outputs": [],
   "source": [
    "14. Which node generated the largest number of KERNRTSP events?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5c5ec8b-2820-44c0-826c-ad3fda0cc37e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node with the largest number of KERNRTSP events:\n",
      "+-------------------+-----+\n",
      "|               node|count|\n",
      "+-------------------+-----+\n",
      "|R63-M0-NE-C:J12-U01|   22|\n",
      "+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, regexp_extract\n",
    "\n",
    "# Configuration variables\n",
    "LOG_FILE_PATH = \"file:///home/hduser/notebooks/BGL.log\"  # Path to the log file\n",
    "APP_NAME = \"NodeWithMaxKERNRTSP\"\n",
    "SPARK_MASTER = \"local[*]\"\n",
    "SPARK_DRIVER_HOST = \"localhost\"\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(APP_NAME) \\\n",
    "    .config(\"spark.driver.host\", SPARK_DRIVER_HOST) \\\n",
    "    .master(SPARK_MASTER) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read the log file into an RDD\n",
    "log_rdd = spark.sparkContext.textFile(LOG_FILE_PATH)\n",
    "\n",
    "# Function to extract node from KERNRTSP log entry\n",
    "def extract_node_from_kernrtsp(line):\n",
    "    if 'KERNRTSP' in line:  # Check if the line contains 'KERNRTSP'\n",
    "        columns = line.split()  # this split the line into columns\n",
    "        if len(columns) > 3:  # it ensure there are enough columns\n",
    "            node = columns[3]  # Extract the node information\n",
    "            return (node, 1)  # Return a tuple with node and count 1\n",
    "    return None  # Return None if not a valid KERNRTSP entry\n",
    "\n",
    "# Extract nodes from KERNRTSP log entries\n",
    "nodes_rdd = log_rdd.map(extract_node_from_kernrtsp).filter(lambda x: x is not None)\n",
    "\n",
    "# Count occurrences of KERNRTSP events per node\n",
    "node_counts_rdd = nodes_rdd.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Find the node with the maximum KERNRTSP events\n",
    "node_with_max_kernrtsp = node_counts_rdd.takeOrdered(1, key=lambda x: -x[1])\n",
    "\n",
    "# Convert the result to a DataFrame\n",
    "node_with_max_kernrtsp_df = spark.createDataFrame(node_with_max_kernrtsp, [\"node\", \"count\"])\n",
    "\n",
    "# Print the result\n",
    "print(\"Node with the largest number of KERNRTSP events:\")\n",
    "node_with_max_kernrtsp_df.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f02712-a86c-4238-afc7-0e0a0fe0587e",
   "metadata": {},
   "outputs": [],
   "source": [
    "18. On which date and time was the earliest fatal kernel error where the message contains ”Lustre mount FAILED”?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2c45e74-3299-4366-a514-1666473b69b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:=======================================>                   (4 + 2) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Earliest fatal kernel error with 'Lustre mount FAILED': 2005-08-03 15:35:34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, regexp_extract, to_timestamp\n",
    "\n",
    "# Initialize SparkSession with specific configurations for memory and cores\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BGLLogAnalysis\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read the log data from the text file\n",
    "log_data = spark.read.text(\"BGL.log\")\n",
    "\n",
    "# Define regex patterns for \"Lustre mount FAILED\" and \"FATAL\"\n",
    "lustre_mount_failed_pattern = r'(?i)Lustre mount FAILED'\n",
    "fatal_pattern = r'(?i)FATAL'\n",
    "\n",
    "# Filter log entries that contain both \"Lustre mount FAILED\" and \"FATAL\"\n",
    "fatal_lustre_errors = log_data.filter(\n",
    "    col(\"value\").rlike(lustre_mount_failed_pattern) & col(\"value\").rlike(fatal_pattern)\n",
    ")\n",
    "\n",
    "# this section will extract datetime from the log entries and convert to timestamp\n",
    "fatal_lustre_errors = fatal_lustre_errors.withColumn(\n",
    "    \"datetime\", regexp_extract(col(\"value\"), r'(\\d{4}-\\d{2}-\\d{2}-\\d{2}\\.\\d{2}\\.\\d{2})', 1)\n",
    ").withColumn(\n",
    "    \"datetime\", to_timestamp(col(\"datetime\"), \"yyyy-MM-dd-HH.mm.ss\")\n",
    ")\n",
    "\n",
    "# Find the earliest datetime entry from the filtered log entries\n",
    "earliest_error = fatal_lustre_errors.orderBy(\"datetime\").first()\n",
    "\n",
    "# Print the earliest fatal error datetime, if any\n",
    "if earliest_error:\n",
    "    print(f\"Earliest fatal kernel error with 'Lustre mount FAILED': {earliest_error['datetime']}\")\n",
    "else:\n",
    "    print(\"No fatal kernel errors with 'Lustre mount FAILED' found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0150ae4-9ec5-4753-91c2-80f50be83e9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
